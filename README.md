# Math Trainer — LLM‑Only Question Generator (with MCQs)

A tiny, fun math trainer for the browser. It generates **brand‑new, LLM‑written questions**—complete with **MCQs (4 options, one correct)**, playful **intros/closers + emojis**, and an **LLM sample solution** on demand. The app stores up to **8 recent questions** and guarantees a **fresh question on every Next click** for the same (area, level, grade, sport) combo.

---

## ✨ Features

* **LLM‑only generation**: No local templates. Every question is produced by the model.
* **MCQs**: 4 plausible options; exactly one correct.
* **Sample answers**: Generated by the LLM when you click **Show Sample Answer** (or pre‑bundled with the question).
* **Freshness**: De‑dup logic so repeated generations for the same combo produce different word problems.
* **Capped history**: Keeps **latest 8** questions in the current session.
* **Playful UX**: Random intros, closers, and sport emoji themes.
* **Resilient JSON parsing**: Handles code‑fences/wrap; retries the bridge; clearer error messages.

---

## 🧩 How it works (high level)

1. User selects **Area, Difficulty, Grade, Sport** and clicks **Generate**.
2. Frontend calls the LLM with a **prompt‑tuned system guide + few‑shot examples** and requests a **strict JSON** payload:

   ```json
   {
     "problem": "string (2–4 sentences; no solution text)",
     "choices": ["string", "string", "string", "string"],
     "answer_index": 0,
     "solution_html": "string (short HTML, 3–6 lines, ends with <strong>Answer: …</strong>)"
   }
   ```
3. UI renders the question, MCQs, and meta. “Next” **always** asks the LLM for a new question for the current combo. The app keeps the latest **8**.
4. Clicking **Show Sample Answer** renders `solution_html` (or asks the LLM to solve if not provided).

---

## 🗂 Project structure (suggested)

```
public/
  index.html        # your UI (form + buttons + containers)
  styles.css        # your styles (buttons, choice states, etc.)
  script.js         # the logic in this repo (LLM calls, rendering, state)
server/
  api-next.js       # optional Node/Express bridge for LLM providers
.env                # provider API keys (if you host your own bridge)
```

> The frontend expects a global `window.LLM.ask` function. You can implement it in the browser or proxy through a backend route like `/api/next`.

---

## 🚀 Quick start

### 1) Install & run a minimal backend bridge (Node + Express)

> If you already expose `window.LLM.ask`, you can skip this and wire your existing function.

Create `server/api-next.js` with something like:

```js
import express from 'express';
import fetch from 'node-fetch';

const app = express();
app.use(express.json({ limit: '1mb' }));

// Example: proxy to an OpenAI/Groq-compatible endpoint
app.post('/api/next', async (req, res) => {
  try {
    const { messages } = req.body; // chat array OR joined prompt string

    // Transform to your provider format:
    const payload = Array.isArray(messages)
      ? { model: process.env.LLM_MODEL, messages }
      : { model: process.env.LLM_MODEL, messages: [{ role: 'user', content: String(messages || '') }] };

    const r = await fetch(process.env.LLM_BASE_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.LLM_API_KEY}`
      },
      body: JSON.stringify(payload)
    });

    if (!r.ok) {
      const text = await r.text();
      return res.status(502).send(`Upstream error: ${r.status} ${text}`);
    }

    const data = await r.json();
    // Adapt this to your provider's response shape; return **string** to the client
    const text = data?.choices?.[0]?.message?.content || data?.choices?.[0]?.text || '';
    return res.send(typeof text === 'string' ? text : JSON.stringify(text));
  } catch (err) {
    return res.status(500).send(`Proxy failure: ${err?.message || 'unknown'}`);
  }
});

app.get('/api/health', (_, res) => res.json({ ok: true }));

const port = process.env.PORT || 8787;
app.listen(port, () => console.log(`LLM bridge on http://localhost:${port}`));
```

Add a tiny dev script to `package.json`:

```json
{
  "type": "module",
  "scripts": {
    "bridge": "node server/api-next.js",
    "dev": "serve public -l 5173" // or any static server you prefer
  }
}
```

Environment variables (example):

```
LLM_BASE_URL=https://api.openai.com/v1/chat/completions
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=sk-***
PORT=8787
```

### 2) Wire the frontend to the bridge

In `public/index.html` (or a small inline script), expose `window.LLM.ask`:

```html
<script>
  window.LLM = {
    ask: async (messagesOrString) => {
      const r = await fetch('/api/next', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ messages: messagesOrString })
      });
      return r.ok ? r.text() : Promise.reject(new Error(await r.text()));
    }
  };
</script>
```

### 3) Drop in the app logic

Place the provided **`script.js`** in `public/` and ensure your HTML has the expected element IDs:

* `generator-form`, inputs: `topic`, `hobby`, `difficulty`, `grade`, `qty`
* display: `problem`, `q-meta`, `choices`, `answer`, `status`
* buttons: `btn-next`, `btn-prev`, `btn-clear`, `btn-eval`, `btn-sample`, `btn-copy`, `btn-reset`

Run the backend and the static server:

```
npm run bridge
npm run dev
```

Open your site (e.g., `http://localhost:5173`) and generate some questions 🎉

---

## 🔧 Configuration & customization

### Max questions

* The app enforces **max 5** questions in memory.
* Change via `MAX_QUESTIONS` in `script.js`.

### Fresh question on Next

* The **Next** button **always** calls the LLM for a new problem for the current combo.
* De‑dup logic keeps a per‑combo set of recently seen problems and retries up to 3 times if the LLM repeats.

### Sports, intros, closers

* Edit the `SPORTS`, `INTROS`, and `CLOSERS` arrays in `script.js` to match your vibe.

### Prompt tuning

* `SYSTEM_GUIDE` and few‑shot examples steer the LLM.
* You can push the model toward more numeric answers, specific formats, or stricter difficulty by adjusting these.

### MCQ evaluation

* A selection is **correct** if it matches the option at `answer_index`.
* For open‑response variants, you can swap evaluation logic to keyword/regex scoring.

### Persistence

* Lightweight history is stored in `localStorage`:

  * Attempts: `mathTrainerHistoryV4`
  * Learned snippets: `trainerLearnedV3` (used to avoid repeats and nudge style)

---

## 🧪 JSON contract (returned by the LLM)

**Required** fields per question:

* `problem` *(string)* — 2–4 sentences; no solution text.
* `choices` *(array of 4 strings)* — plausible, distinct, similar magnitude.
* `answer_index` *(0–3)* — index into `choices`.
* `solution_html` *(string)* — short, step‑by‑step HTML (3–6 lines), ends with `<strong>Answer: …</strong>`.

**Example**

```json
{
  "problem": "A mini tennis match uses 4 points per game. Zara wins some games with a total of 28 points while her opponent has 16. If Zara won 5 games, how many games did her opponent win?",
  "choices": ["2", "3", "4", "5"],
  "answer_index": 1,
  "solution_html": "Each game = 4 pts.<br>Zara: 5 games → 20 pts, remainder 8 from other games implies total games = (28+16)/4 = 11.<br>Opponent games = 11 - 5 - (Zara’s partials already counted in total); compute carefully per scoring scheme (adjust narrative).<br><strong>Answer: 3</strong>"
}
```

> Your model will produce domain‑consistent narratives; the client validates shape (not correctness) and uses `answer_index` for evaluation.

---

## 🛟 Troubleshooting

* **“LLM error: Empty response”**

  * Ensure your backend returns **plain text** (the model’s content) to the client.
  * Check `/api/health` (should be `{ ok: true }`).
  * Confirm `window.LLM.ask` exists and returns a **string**.
* **“LLM did not return valid JSON.”**

  * The parser auto‑strips code fences and hunts JSON blocks, but keep your system prompt strict about JSON‑only output.
* **CORS / Mixed content**

  * If the bridge is on a different origin/port, enable CORS and use HTTPS consistently.
* **Rate limits**

  * Add backoff on the server, and show friendlier UI messages if you’re throttled.

---

## 🔐 Security & privacy

* The app keeps lightweight history in the browser only.
* Don’t send personal data to the LLM.
* If you log prompts/responses server‑side, scrub PII and follow your data retention policies.

---

## 🗺 Roadmap ideas

* Timed practice modes
* Per‑topic mastery tracking
* Export attempts as CSV
* Teacher dashboard with class codes

---

## 📄 License

MIT (or your preferred license). Include a `LICENSE` file if distributing.

---

## 🙌 Credits

Built with love for quick math practice. Emojis and intros/closers make it friendly; prompt‑tuning keeps the content consistent and fresh.
